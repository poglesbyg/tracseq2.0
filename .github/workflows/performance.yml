# TracSeq 2.0 Performance Testing Pipeline
name: ðŸš€ Performance Tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
    types: [ labeled ]
  schedule:
    - cron: '0 2 * * 1'  # Weekly on Monday at 2 AM
  workflow_dispatch:
    inputs:
      test_duration:
        description: 'Test duration in seconds'
        required: false
        default: '300'
      concurrent_users:
        description: 'Number of concurrent users'
        required: false
        default: '100'

env:
  RUST_VERSION: '1.75'
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'
  K6_VERSION: '0.47.0'

jobs:
  # ðŸ—ï¸ Setup Test Environment
  setup-environment:
    name: ðŸ—ï¸ Setup Test Environment
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'schedule' ||
      (github.event_name == 'pull_request' && contains(github.event.pull_request.labels.*.name, 'performance')) ||
      (github.event_name == 'push' && github.ref == 'refs/heads/main')
    outputs:
      test_duration: ${{ steps.config.outputs.test_duration }}
      concurrent_users: ${{ steps.config.outputs.concurrent_users }}
    steps:
      - uses: actions/checkout@v4

      - name: Configure test parameters
        id: config
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "test_duration=${{ github.event.inputs.test_duration }}" >> $GITHUB_OUTPUT
            echo "concurrent_users=${{ github.event.inputs.concurrent_users }}" >> $GITHUB_OUTPUT
          else
            echo "test_duration=300" >> $GITHUB_OUTPUT
            echo "concurrent_users=100" >> $GITHUB_OUTPUT
          fi

  # ðŸ¦€ Rust Service Performance
  rust-performance:
    name: ðŸ¦€ Rust Service Performance
    needs: setup-environment
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service:
          - auth_service
          - sample_service
          - enhanced_storage_service
    
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_USER: postgres
          POSTGRES_DB: perf_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - uses: actions/checkout@v4

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: ${{ env.RUST_VERSION }}

      - name: Build service in release mode
        working-directory: lims-core/${{ matrix.service }}
        run: |
          cargo build --release
          
      - name: Start service
        working-directory: lims-core/${{ matrix.service }}
        env:
          DATABASE_URL: postgres://postgres:postgres@localhost:5432/perf_test
        run: |
          # Run migrations if they exist
          if [ -d "migrations" ]; then
            cargo install sqlx-cli --no-default-features --features postgres,rustls
            sqlx database create || true
            sqlx migrate run
          fi
          
          # Start the service in background
          ./target/release/${{ matrix.service }} &
          SERVICE_PID=$!
          echo "SERVICE_PID=$SERVICE_PID" >> $GITHUB_ENV
          
          # Wait for service to be ready
          sleep 10

      - name: Run performance tests
        run: |
          # Install k6
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6
          
          # Create k6 test script
          cat > loadtest.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          
          export let options = {
            stages: [
              { duration: '1m', target: 10 },
              { duration: '3m', target: ${{ needs.setup-environment.outputs.concurrent_users }} },
              { duration: '1m', target: 0 },
            ],
            thresholds: {
              http_req_duration: ['p(95)<500'],
              http_req_failed: ['rate<0.1'],
            },
          };
          
          export default function() {
            let response = http.get('http://localhost:3000/health');
            check(response, {
              'status is 200': (r) => r.status === 200,
            });
            sleep(1);
          }
          EOF
          
          # Run k6 test
          k6 run --out json=results_${{ matrix.service }}.json loadtest.js || true

      - name: Stop service
        if: always()
        run: |
          if [ ! -z "$SERVICE_PID" ]; then
            kill $SERVICE_PID || true
          fi

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: performance-results-${{ matrix.service }}
          path: results_${{ matrix.service }}.json

  # ðŸ Python Service Performance
  python-performance:
    name: ðŸ Python Service Performance
    needs: setup-environment
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - service: api_gateway
            path: lims-core/api_gateway
          - service: lab_submission_rag
            path: lims-ai/lab_submission_rag

    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_USER: postgres
          POSTGRES_DB: perf_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        working-directory: ${{ matrix.path }}
        run: |
          pip install -r requirements.txt
          pip install gunicorn

      - name: Start service
        working-directory: ${{ matrix.path }}
        env:
          DATABASE_URL: postgres://postgres:postgres@localhost:5432/perf_test
        run: |
          # Start the service with gunicorn
          gunicorn -w 4 -b 0.0.0.0:8000 main:app --daemon
          
          # Wait for service to be ready
          sleep 10

      - name: Run performance tests with Locust
        run: |
          pip install locust
          
          # Create Locust test file
          cat > locustfile.py << 'EOF'
          from locust import HttpUser, task, between
          
          class APIUser(HttpUser):
              wait_time = between(1, 3)
              
              @task
              def health_check(self):
                  self.client.get("/health")
              
              @task(3)
              def api_endpoint(self):
                  self.client.get("/api/v1/status")
          EOF
          
          # Run Locust test
          locust \
            --host=http://localhost:8000 \
            --users=${{ needs.setup-environment.outputs.concurrent_users }} \
            --spawn-rate=10 \
            --run-time=${{ needs.setup-environment.outputs.test_duration }}s \
            --headless \
            --html=locust_report_${{ matrix.service }}.html

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: locust-results-${{ matrix.service }}
          path: locust_report_${{ matrix.service }}.html

  # ðŸŽ¨ Frontend Performance
  frontend-performance:
    name: ðŸŽ¨ Frontend Performance
    needs: setup-environment
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup pnpm
        uses: pnpm/action-setup@v3
        with:
          version: '10.12.2'

      - name: Install and build frontend
        working-directory: lims-ui
        run: |
          pnpm install --frozen-lockfile
          pnpm build

      - name: Run Lighthouse CI
        run: |
          npm install -g @lhci/cli
          
          # Start a simple HTTP server
          cd lims-ui/dist
          python -m http.server 8080 &
          SERVER_PID=$!
          
          # Wait for server to start
          sleep 5
          
          # Run Lighthouse
          lhci autorun \
            --collect.url=http://localhost:8080 \
            --collect.numberOfRuns=3 \
            --assert.preset=lighthouse:recommended \
            --upload.target=temporary-public-storage || true
          
          # Stop server
          kill $SERVER_PID

  # ðŸ“Š Performance Analysis
  performance-analysis:
    name: ðŸ“Š Performance Analysis
    needs: [rust-performance, python-performance, frontend-performance]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4

      - name: Analyze results
        run: |
          echo "## ðŸš€ Performance Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### Test Configuration:" >> $GITHUB_STEP_SUMMARY
          echo "- Duration: ${{ needs.setup-environment.outputs.test_duration }}s" >> $GITHUB_STEP_SUMMARY
          echo "- Concurrent Users: ${{ needs.setup-environment.outputs.concurrent_users }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### Test Results:" >> $GITHUB_STEP_SUMMARY
          echo "| Service | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Rust Services | ${{ needs.rust-performance.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Python Services | ${{ needs.python-performance.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Frontend | ${{ needs.frontend-performance.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Analyze k6 results if available
          for file in performance-results-*/results_*.json; do
            if [ -f "$file" ]; then
              echo "### Results from $file:" >> $GITHUB_STEP_SUMMARY
              # Basic analysis would go here
              echo "- Results file found" >> $GITHUB_STEP_SUMMARY
            fi
          done
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Performance Recommendations:" >> $GITHUB_STEP_SUMMARY
          echo "- Monitor p95 latencies for all services" >> $GITHUB_STEP_SUMMARY
          echo "- Ensure error rates stay below 1%" >> $GITHUB_STEP_SUMMARY
          echo "- Review any endpoints with high latency" >> $GITHUB_STEP_SUMMARY
          echo "- Consider caching for frequently accessed data" >> $GITHUB_STEP_SUMMARY

      - name: Create performance report
        run: |
          # Create a consolidated performance report
          cat > performance-report.md << 'EOF'
          # TracSeq 2.0 Performance Test Report
          
          ## Test Environment
          - Date: $(date)
          - Branch: ${{ github.ref }}
          - Commit: ${{ github.sha }}
          
          ## Results Summary
          Performance testing completed for all services.
          
          ### Next Steps
          1. Review individual service reports
          2. Identify performance bottlenecks
          3. Optimize critical paths
          4. Re-run tests after optimizations
          EOF

      - name: Upload consolidated report
        uses: actions/upload-artifact@v4
        with:
          name: performance-report
          path: performance-report.md
