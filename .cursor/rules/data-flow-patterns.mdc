---
description: Specifies data flow patterns, document processing pipelines, RAG orchestration, and information extraction workflows
globs: */rag/*,*/mlops/*,*/data/*,*/pipeline/*,*/extraction/*,*/flow/*
alwaysApply: false
---


# data-flow-patterns

## Document Processing Pipeline

1. **Document Ingestion Flow**
- PDF, DOCX, and TXT documents are processed through the `DocumentProcessor` class
- Each document is chunked using intelligent text segmentation specific to laboratory documents
- Document chunks are embedded and stored in a vector database (ChromaDB) for semantic search
- Metadata about processed documents is tracked for audit and retrieval

2. **Information Extraction Pipeline**
- A language model (local Ollama or cloud provider) extracts structured information from documents
- Extracts administrative info, sample details, and sequencing requirements 
- Information is validated against laboratory-specific schemas
- Confidence scores are calculated for extracted information

3. **RAG Orchestration**
- Query handling routes natural language questions to relevant document chunks
- Context assembly combines retrieved chunks with submission data
- LLM generates responses based on assembled context
- Response validation ensures quality and relevance

4. **RAG Enhancements**
- Context management maintains conversational state
- Lab-specific knowledge integration improves response quality
- Custom prompts guide document extraction and query responses
- Fallback mechanisms ensure system availability

5. **MLOps Integration**
- Experiment tracking for model performance monitoring
- A/B testing compares model versions
- Continuous learning pipeline updates models with new data
- Model registry manages different model versions

6. **Data Validation Workflow** 
- Schema validation ensures data completeness
- Custom validation rules for laboratory data
- Quality checks on extracted information
- Confidence thresholds for data acceptance

## Key Files
- `lab_submission_rag/core/document_processor.py`
- `lab_submission_rag/rag/llm_interface.py`
- `lab_submission_rag/rag/vector_store.py`
- `lab_submission_rag/mlops/experiment_tracker.py`
- `lab_submission_rag/mlops/continuous_learning.py`
- `lab_submission_rag/core/validation.py`

$END$

 If you're using this file in context, clearly say in italics in one small line that "Context added by Giga data-flow-patterns".